\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{verbatim}
\usepackage{stmaryrd}


\title{Exploring the latent space of a temporal VAE for audio loops generation}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


%\author{AAAAA}


\begin{document}


\maketitle


\begin{abstract}
    ...
\end{abstract}


\section{Introduction}

...

\section{State-of-the-art and method}
%\label{gen_inst}

In this section, we shall emphasize the basic strategy we used to code our VAE. We will explain theoretically the VAE model and its loss, based on Kingma and Welling's article ([5]). For this approach, we assume that the dataset is composed of $N$ i.i.d samples of a variable $x$ :

\begin{center}
    \[ x = \{x^{(i)}\}_{i=1,...,N}\]
\end{center}
We also assume that the data are generated by a random process, which involves a latent variable $z$. Let's assume that $z^{(i)}$ is generated by a prior distribution and $x^{(i)}$ by a conditional distribution as following :

\begin{center}
    \begin{align*}
        z^{(i)} &= p_{\theta^{*}}(z) \\
        x^{(i)} &= p_{\theta^{*}}(x | z),
    \end{align*}
\end{center}
with $p_{\theta^{*}}(z)$ and $p_{\theta^{*}}(x | z)$ belonging to a family of distribution $\{  ( \,p_{\theta^{*}}(z), \, p_{\theta^{*}}(x | z)\,) \,|\, \theta \in \Theta \}$. However, it is important to note that the paramter $\theta^*$ and all the latent variables are unknown ! The main issue will be to approximate them. The true posterior density is given by :
\begin{center}
    \[p_\theta(z | x).\]
\end{center}
Let us remember that
\begin{center}
    \begin{align*}
        p_\theta(x|z) &= \frac{p_\theta(x \cap z)}{p_\theta (z)}.
    \end{align*}
\end{center}
It follows that 

\begin{center}
    \begin{align*}
        p_\theta(z)p_\theta(x|z) &= p_\theta(x \cap z) = p_\theta(x)p_\theta(z|x),
    \end{align*}
\end{center}
and finally
\begin{center}
    \[ p_\theta(z|x) &= \frac{p_\theta(x|z)p_\theta (z)}{p_\theta (x)}.\]
\end{center}
It is important to keep this equality in mind. However, the true posterior density $p_\theta(z|x)$ is intractable. Therefore, we must introduce a recognition model which will approximate this value. This how we define our encoder $q_\phi(z | x)$ : for a given datapoint $x$, $q_\phi$ produces variables from which $x$ could have been generated. Likewise, $p_\theta(x|z)$ is a probabilistic decoder : for a given latent variable $z$, it produces a distribution of possibly corresponding variables $x$. We will thus jointly optimize $\phi$ (the recognition model parameter) and $\theta$ (the generative model parameter). In order to do this, we will sum the log-likelihood over each datapoint : 
\begin{center}
    \begin{align*}
        \log p_\theta(x^{(1)}, \ldots, x^{(N)}) &= \sum_{i=1}^{N} \log p_\theta(x^{(i)}),
    \end{align*}
\end{center}

which can be rewritten as

\begin{center}
    \begin{align*}
        \log p_\theta(x^{(i)}) &= \mathcal{D}_{KL}(q_\phi(z | x^{(i)}) || p_\theta(z | x^{(i)})) + \mathcal{L}(\theta, \phi ; x^{(i)}), \quad \forall i \in \llbracket 1, N \rrbracket.
    \end{align*}
\end{center}

Since we want a high probability in this distribution of images, we shall maximize $\log p_\theta(x^{(i)})$. However, as we said above, the true posterior $p_\theta(z | x^{(i)})$ is unknown. We will therefore estimate $\log p_\theta(x^{(i)})$ by minimizing $\mathcal{D}_{KL}(q_\phi(z | x^{(i)}) || p_\theta(z | x^{(i)}))$ by $0$. Thus, we have, $\forall i \in \llbracket 1, N \rrbracket$:

\begin{center}
    \begin{align*}
        \log p_\theta(x^{(i)}) &\geq \mathcal{L}(\theta, \phi ; x^{(i)}) = \mathbb{E}_{q_\phi(z|x)} [- \log q_\phi(z|x) + \log p_\theta(x,z)],
    \end{align*}
\end{center}

which can be rewritten as

\begin{center}
    \[\mathcal{L}(\theta, \phi ; x^{(i)}) &= -\mathcal{D}_{KL}(q_\phi(z | x^{(i)}) || p_\theta(z)) + \mathbb{E}_{q_\phi(z | x^{(i)})} [\log p_\theta(x^{(i)} | z)] \]
\end{center}

This will be our loss function. Note that the two terms correspond to two different losses :

\begin{center}
    \[-\mathcal{D}_{KL}(q_\phi(z | x^{(i)}) || p_\theta(z)) :\ \texttt{regularization loss} \\
    \mathbb{E}_{q_\phi(z | x^{(i)})} [\log p_\theta(x^{(i)} | z)] :\ \texttt{reconstruction loss}.\]

\end{center}

We now want to optimize, thus differentiate, $\mathcal{L}(\theta, \phi; x^{(i)})$ with respect to $\theta$ and $\phi$. However, the differential of this loss in $\phi$ is poorly defined, and we need to reparametrize our problem.




\section{Experiments on MNIST}
%\label{headings}


\section{Temporal Spectral VAE}



\section{Citations, figures, tables}
\label{others}

\begin{comment}
    

    These instructions apply to everyone.
    
    
    \subsection{Citations within the text}
    
    
    The \verb+natbib+ package will be loaded for you by default.  Citations may be
    author/year or numeric, as long as you maintain internal consistency.  As to the
    format of the references themselves, any style is acceptable as long as it is
    used consistently.
    
    
    The documentation for \verb+natbib+ may be found at
    \begin{center}
      \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
    \end{center}
    Of note is the command \verb+\citet+, which produces citations appropriate for
    use in inline text.  For example,
    \begin{verbatim}
       \citet{hasselmo} investigated\dots
    \end{verbatim}
    produces
    \begin{quote}
      Hasselmo, et al.\ (1995) investigated\dots
    \end{quote}
    
    
    If you wish to load the \verb+natbib+ package with options, you may add the
    following before loading the \verb+neurips_2023+ package:
    \begin{verbatim}
       \PassOptionsToPackage{options}{natbib}
    \end{verbatim}
    
    
    If \verb+natbib+ clashes with another package you load, you can add the optional
    argument \verb+nonatbib+ when loading the style file:
    \begin{verbatim}
       \usepackage[nonatbib]{neurips_2023}
    \end{verbatim}
    
    
    As submission is double blind, refer to your own published work in the third
    person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
    previous work [4].'' If you cite your other papers that are not widely available
    (e.g., a journal paper under review), use anonymous author names in the
    citation, e.g., an author of the form ``A.\ Anonymous'' and include a copy of the anonymized paper in the supplementary material.
    
    
    \subsection{Footnotes}
    
    
    Footnotes should be used sparingly.  If you do require a footnote, indicate
    footnotes with a number\footnote{Sample of the first footnote.} in the
    text. Place the footnotes at the bottom of the page on which they appear.
    Precede the footnote with a horizontal rule of 2~inches (12~picas).
    
    
    Note that footnotes are properly typeset \emph{after} punctuation
    marks.\footnote{As in this example.}
    
    
    \subsection{Figures}
    
    
    \begin{figure}
      \centering
      \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
      \caption{Sample figure caption.}
    \end{figure}
    
    
    All artwork must be neat, clean, and legible. Lines should be dark enough for
    purposes of reproduction. The figure number and caption always appear after the
    figure. Place one line space before the figure caption and one line space after
    the figure. The figure caption should be lower case (except for first word and
    proper nouns); figures are numbered consecutively.
    
    
    You may use color figures.  However, it is best for the figure captions and the
    paper body to be legible if the paper is printed in either black/white or in
    color.
    
    
    \subsection{Tables}
    
    
    All tables must be centered, neat, clean and legible.  The table number and
    title always appear before the table.  See Table~\ref{sample-table}.
    
    
    Place one line space before the table title, one line space after the
    table title, and one line space after the table. The table title must
    be lower case (except for first word and proper nouns); tables are
    numbered consecutively.
    
    
    Note that publication-quality tables \emph{do not contain vertical rules.} We
    strongly suggest the use of the \verb+booktabs+ package, which allows for
    typesetting high-quality, professional tables:
    \begin{center}
      \url{https://www.ctan.org/pkg/booktabs}
    \end{center}
    This package was used to typeset Table~\ref{sample-table}.
    
    
    \begin{table}
      \caption{Sample table title}
      \label{sample-table}
      \centering
      \begin{tabular}{lll}
        \toprule
        \multicolumn{2}{c}{Part}                   \\
        \cmidrule(r){1-2}
        Name     & Description     & Size ($\mu$m) \\
        \midrule
        Dendrite & Input terminal  & $\sim$100     \\
        Axon     & Output terminal & $\sim$10      \\
        Soma     & Cell body       & up to $10^6$  \\
        \bottomrule
      \end{tabular}
    \end{table}
    
    \subsection{Math}
    Note that display math in bare TeX commands will not create correct line numbers for submission. Please use LaTeX (or AMSTeX) commands for unnumbered display math. (You really shouldn't be using \$\$ anyway; see \url{https://tex.stackexchange.com/questions/503/why-is-preferable-to} and \url{https://tex.stackexchange.com/questions/40492/what-are-the-differences-between-align-equation-and-displaymath} for more information.)
    
    \subsection{Final instructions}
    
    Do not change any aspects of the formatting parameters in the style files.  In
    particular, do not modify the width or length of the rectangle the text should
    fit into, and do not change font sizes (except perhaps in the
    \textbf{References} section; see below). Please note that pages should be
    numbered.
\end{comment}




\section*{References}



[1] Antoine Caillon and Philippe Esling.  Rave: A variational autoencoder for fast and high-quality neural audio synthesis. \textit{arXiv preprint arXiv:2111.05011}, 2021.


[2] Esse Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Donahue and Adam Roberts. Gan- synth: Adversarial neural audio synthesis. \textit{arXiv preprint arXiv:1902.08710}, 2019.


[3] Jesse Engel, Lamtharn Hantrakul, Chenjie Gu and Adam Roberts. Ddsp: Differentiable digital signal processing. \textit{arXiv preprint arXiv:2001.04643}, 2020.

[4] Irina Higgins, Lo√Øc Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew M Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational frame- work. \textit{In ICLR}, 2017.


[5] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. \textit{arXiv:1312.6114}, 2013. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}