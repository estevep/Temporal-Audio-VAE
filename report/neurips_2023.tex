\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[nonatbib, final]{neurips_2020}
\usepackage{subcaption}

\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[final]{neurips_2020}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}




% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option :
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors








\usepackage{verbatim}
\usepackage{stmaryrd}
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{interval}
\usepackage{bm}

\lstdefinestyle{mystyle}{
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=false,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}




\title{Projet ATIAM 2022-2023 : Exploring the latent space of a temporal VAE for audio loops generation}
\author{Étienne André, Paul Estève, Antoine Souchaud \& Enguérand Tamagna}
%\title{Exploring the latent space of a temporal VAE for audio loops generation}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


%\author{AAAAA}
\usepackage[
backend=biber,
style=numeric,
sorting=ynt
]{biblatex}

\addbibresource{biblio.bib}
% \AtEveryBibitem{%
% \ifentrytype{book,collection,incollection,misc}{
%     \clearfield{url}%
%     \clearfield{urldate}%
%     \clearfield{review}%
%     \clearfield{series}%%
%     \clearfield{primaryclass}
% }{}
% }
% Exclure le champ primaryclass
\AtEveryBibitem{\clearfield{primaryclass}}
% }
\date{\today}

\begin{document}


\maketitle
\vfill
\hspace*{-3cm}\rotatebox[origin=l]{90}{\vspace*{-10cm}\textcolor{gray}{\Huge\today}}\hspace*{1cm}


\begin{abstract}
    Synthesis of complex and high-quality audio has been a thoroughly explored research field, usually using signal processing based techniques. In recent developments, machine learning based techniques attain a high quality and diverse generation, but at the cost of computational complexity and loss of control over the timbral and temporal characteristics of the generated sounds. Approaches such as Variational Auto-Encoders \cite{kingmaAutoEncodingVariationalBayes2022} rely on the generation of a low-dimensional latent space in which the characteristic information of the data is encoded. Decoding this space provides a trainable analysis-synthesis framework. More recently, the Realtime Audio Variational autoEncoder (RAVE) paper\cite{caillonRAVEVariationalAutoencoder2021} shows significant improvements over computational complexity, allowing high-quality and realtime on CPUs. However, being based on waveforms, it presents challenges in the structure and interpretability of the latent dimensions. In this paper, we propose a way to improve on these issues by introducing a mel-spectrogram representation, and discuss the impact on the latent space structure and reconstruction quality.
\end{abstract}

% Exploring the latent space of a temporal VAE for audio loops generation

% abstract 

% 3 - Proposed methode

% 3.1 Audio representation (mel spec + griffin lim / pghi)

% 3.2 Model


% 4 - experiments on MNIST


% 5 - Experiments

% - dataset
% - baseline = pretrained RAVE
% - parameters you tried (for instance maybe you change the number of latent)
% - metrics (for reconstruction quality for instance) ?


% 7 - Conclusion


% \begin{comment}
\section{Introduction}

There have been significant advances in deep audio generative models in recent years, enabling novel and artistically interesting ways to generate diverse and high-quality sounds. By exposing a deep neural network to a set of examples, the network is able to model the underlying data distribution and generate coherent and rich new sounds.
However, the high dimensionality of audio signals, due to the high rates at which they are sampled, is challenging to model directly. Early works on autoregressive models such as WaveNet\cite{oord2016wavenet} achieve high-quality waveform synthesis, but at the cost of high computational complexity and slow inference. Alternatively, other approaches that leverage a time-frequency representation, such as GANSynth\cite{engelGANSynthAdversarialNeural2019}, are able to reduce the computational cost but rely on very large architectures with a high number of parameters.

These approaches usually rely on latent-based generative models such as Variational Auto-Encoders (VAE)\cite{kingmaAutoEncodingVariationalBayes2022} and Generative Adversarial Networks (GAN)\cite{kumarMelGANGenerativeAdversarial2019}, which use a probabilistic framework to model the underlying probability distribution. To do so, they introduce a lower-dimension so-called latent space, which captures the high-level audio features of the dataset. The latent variables in this parametric space are assumed to be responsible for most of the input variations, and can then be used to condition the generation.

More recently, Realtime Audio Variational autoEncoder (RAVE) \cite{caillonRAVEVariationalAutoencoder2021} combines these approaches to achieve both high-quality synthesis and low computational complexity, with a very reduced latent space dimensionality, enabling high-quality and diverse generation even in resource-constrained systems and realtime applications. However, the latent space, although small, lacks interpretability. Indeed, two perceptually similar sounds may have completely different waveforms due to subtle phase variations: thus, similar audio features may end up being encoded very differently in the latent space, making it difficult to explore in creative endeavors.

In this paper, we address this issue by introducing a time-frequency based representation in the learning phase of RAVE. To do so, we implement a temporal spectral VAE using mel-spectrograms, eliminating phase-related issues in the latent space. We start by preseting our method in detail, and discuss the influence of the model's hyperparameters by presenting experiments on the MNIST dataset\cite{lecun2010mnist}. We then assess the reconstruction quality and latent space interpretability of our model trained on a pruned Freesound Loop dataset\cite{ramires2020}, and compare it with a pre-trained RAVE model on the same dataset. We finally discuss our results and explain the difficulties encountered.

Full source code, audio samples and a pre-trained checkpoint are availiable on the accompanying repository\footnote{\url{https://github.com/estevep/Temporal-Audio-VAE}}.

% SARAH
% In recent years, significant advances in deep audio generative models. 
% The goal is to model the underlying data distribution by learning from a set of examples. 
% Directly modeling the raw waveform is challenging as audio data is very high dimensional
% Early works on Autoregressive (AR) models such as WaveNet successfully achieved high-quality raw waveform synthesis, but at the cost of expensive computation and very slow inference. Alternatively, other approaches leverage a time-frequency representation of the signals such as GANSynth, reducing the computational cost but rely on very large architectures requiring lots of parameters to train. 
% These approaches mostly rely on latent-based generative models such as VAE and GAN.
% To model the underlying distribution, they introduce latent variables assumed to be responsible for most of the input variations in a lower-dimensional space called latent space. This parametric space can capture high-level audio features that can then be used to condition the generation. 
% Recently RAVE was introduced, it combines VAE+GAN and achieve fast and high-quality synthesis + embeds the possibility to work with reduced latent spaces (thanks to the post-training analysis)
% (128 latent alors que nous on en a 8, grâce à la post training Analysis)


% However the latent dimensions are usually not directly interpretable. + two perceptually similar sounds may have completely different waveforms due to subtle phase variations. Hence similar audio features could be encoded very differently in the RAVE latent space precluding its exploration for creative endeavors.
% Hence, we want to analyze the benefit of a time-frequency for the representation learning phase of RAVE and its impact on the latent representation. implement a Temporal Spectral VAE and compare with a pre-trained RAVE

%ORIGINAL
% Neural audio synthesis is nowadays an active research domain that opens new doors for creativity, composition and sound design. Generating various high-quality audio samples is now possible thanks to generative models such as RAVE \cite{caillonRAVEVariationalAutoencoder2021}, which are capable of create new data which preserve the properties of the training data. To do so, deep generative approaches such as Variational Auto-Encoders \cite{kingmaAutoEncodingVariationalBayes2022} crate an low-dimensional latent space on which the data are encoded into latent variables. Decoding these then allows us to generate new data. However, the latent space is a theoretical consideration that is very complex to represent since the dimensions involved are still too big to understand. We therefore cannot exert an active and intuitive control over the latent space, nor evaluate the quality of such a space. Our goal will be then to explore and sample the latent space of a pre-trained VAE-based audio synthesis model in order to assess its quality and coverage. In the first place, we will look at the state-of-the-art data generation method. We will then train it on MNIST in order to experiment and discuss qualitative results. Finally, we will dig into the Temporal Spectral VAE properly said.
%Latent space exploration ??



% {\huge Plan}

\section{State-of-the-art}
%\label{gen_inst}
% 2 - State-of-the-art

% STRUCTURE EN ENTONNOIR ! Generative models

% 2.1. Generative models
\subsection{Generative models}

% complete model is defined by the joint distribution p(x,z) = p(x|z)p(z) and we obtain p(x) by marginalizing z from the joint probability.
% however intractable –> maximum likelihood estimation → approximate the likelihood density distribution using neural networks

The purpose of generative models is to create a model for the underlying distribution $p(\textbf{x})$ of a given dataset $\textbf{x} \in \mathbb{R}^{d_x}$. We assume that the data are generated by a random process, which involves a latent variable $\textbf{z} \in \mathbb{R}^{d_z}$ that plays a key role in influencing the majority of the variations present in $\textbf{x}$. Thus, we obtain a joint distribution which define the complete model $p(\textbf{x}, \textbf{z}) = p(\textbf{x}|\textbf{z})p(\textbf{z})$ and we obtain $p(\textbf{x})$ by marginalizing $\textbf{z}$ from the joint probability. 
However, the true posterior density $p(\textbf{z}|\textbf{x})$ is intractable, so
we must use a maximum likelihood estimation to approximate the likelihood density distribution using neural networks.





% 2.2. VAE
\subsection{Variational AutoEncoders (VAE)}

% ELBO
% then introduce beta
% Bien expliquer la partie Elbo (reconstruction) et la partie régularisation

% Encodeur modélise posteriori et décoder log vraisemblance


To solve this problem Kingma and Welling \cite{kingmaAutoEncodingVariationalBayes2022} have introduced Variational AutoEncoders (VAE). The VAE model introduces a recognition model which will approximate the true posterior density. This defines an encoder $q_\phi(\textbf{z} | \textbf{x})$ : for a given datapoint $\textbf{x}$, $q_\phi$ produces variables from which $\textbf{x}$ could have been generated. 

The parameter $\phi$ is supposed to minimize its Kullback-Leibler (KL) divergence with the actual posterior distribution $p(\textbf{z}|\textbf{x})$ through optimization :
\begin{equation}
    \phi^* = \text{argmin}_{\phi} \mathcal{D}_{\text{KL}}[q_\phi(\textbf{z}|\textbf{x}||p(\textbf{z}||\textbf{x}))].
\end{equation}
Likewise, $p_\theta(\textbf{x}|\textbf{z})$ is a probabilistic decoder : for a given latent variable $\textbf{z}$, it produces a distribution of possibly corresponding variables $\textbf{x}$. We will thus jointly optimize $\phi$ (the recognition model parameter) and $\theta$ (the generative model parameter). It can be showed (see anexes) that this optimization problem can be reduced to the final loss used to train a VAE, called Evidence Lower BOund (ELBO) :

\begin{equation}\label{loss1}
    \mathcal{L}(\theta, \phi ; \textbf{x}^{(i)}) = -\mathcal{D}_{KL}(q_\phi(\textbf{z} | \textbf{x}^{(i)}) || p_\theta(\textbf{z})) + \mathbb{E}_{q_\phi(\textbf{z} | \textbf{x}^{(i)})} [\log p_\theta(\textbf{x}^{(i)} | \textbf{z})]
\end{equation}

%on a (\ref{loss1})

Note that the two terms correspond to two different losses :

\begin{align*}
    \mathbb{E}_{q_\phi(\textbf{z} | \textbf{x}^{(i)})} [\log p_\theta(\textbf{x}^{(i)} | \textbf{z})] &:\ \texttt{reconstruction loss} \\
    \mathcal{D}_{KL}(q_\phi(\textbf{z} | \textbf{x}^{(i)}) || p_\theta(\textbf{z})) &:\ \texttt{regularization loss}.
\end{align*}

The reconstruction loss increases the likelihood of the data generated given a configuration of the latent, while the regularization loss measures the proximity of the true prior $p_\theta (\textbf{z})$ compared to the approximated posterior $q_\phi(\textbf{z}|\textbf{x})$. This one allows the algorithm to optimize the choice of $q$ with respect to the true posterior distribution. 

\textit{The reconstruction aims to maximise the mutual information between the latent representation and the data distribution while the regularisation tends to make the posterior distribution independent from data.}

Note that we will use an extra parameter $\beta$ in order to adjust the weight of the regularization loss, as explained by Higgins et al. \cite{higginsVVAELEARNINGBASIC2017} :

\begin{center}
    \begin{equation}\label{eq:loss1beta}
        \mathcal{L}(\theta, \phi ; \textbf{x}^{(i)}) = - \beta \, \mathcal{D}_{KL}(q_\phi(\textbf{z} | \textbf{x}^{(i)}) || p_\theta(\textbf{z})) + \mathbb{E}_{q_\phi(\textbf{z} | \textbf{x}^{(i)})} [\log p_\theta(\textbf{x}^{(i)} | \textbf{z})].
    \end{equation}
\end{center}

They demonstrate that as $\beta > 1$ increases, the latent dimensions become less entangled, but this comes at the expense of the reconstruction quality.

\begin{comment}
    

\begin{figure}[h]
    \centering
    \begin{tikzpicture}[>=Stealth, node distance=0.5cm, scale=0.7]

        % Noeuds (cercles)
        \node [draw, circle, scale=0.9] (A) {Image};
        \node [draw, circle, below=of A, scale=0.6] (B) {Encoder};
        \node [draw, circle, below right=of B, xshift=-2cm, yshift=-0.5cm, scale=1] (C) {$\bm\mu$};
        \node [draw, circle, below left=of B, xshift=2cm, yshift=-0.5cm, scale=1.1] (D) {$\bm\sigma$};
        \node [draw, circle, below=of B, yshift=-1.5cm, scale=0.8] (E) {$\textbf{z} = \bm\mu \, + \, \bm\sigma \odot \bm\epsilon$};
        \node [draw, circle, below left=of C, xshift=0cm, yshift=-1.25cm, scale=1.2] (eps) {$\bm\epsilon$};
        \node [draw, circle, below=of E, scale=0.6] (F) {Decoder};
        \node [draw, circle, below=of F, scale=0.9] (G) {Image};

        % Flèches verticales
        \draw [->] (A) -- (B);
        \draw [->] (B) -- (C);
        \draw [->] (B) -- (D);
        \draw [->] (C) -- (E);
        \draw [->] (D) -- (E);
        \draw [->] (E) -- (F);
        \draw [->] (F) -- (G);
        \draw [->] (eps) -- (E);

        % Flèche courbée de A à G avec un angle plus important
        \draw [<->, bend left=60] (A) to node[midway, right] {$\approx$} (G);

    \end{tikzpicture}
    \caption{Complete diagram of the VAE with reparametrization}
    \label{VAE}
\end{figure}
\end{comment}


% 2.3 RAVE
\subsection{Realtime Audio Variational autoEncoder (RAVE)}

% Utilise un VAE, garde la temporalité pour pouvoir avoir une trajectoire dans l'espace latent.
% 2e phase adversariale qui permet d'améliorer la qualité de la génération (cf annexes)
% Post training analysis en 1 ou 2 phrases, pas besoin de détailler équation -> réduire à 8 latentes
% Phase rend les choses galère 


In order to highlight the characteristic information of the latent space, we can draw inspiration from the Realtime Audio Variational autoEncoder \cite{caillonRAVEVariationalAutoencoder2021}. This uses a VAE while keeping the temporality to be able to have a trajectory in the latent space.

When we code a VAE, the representation learned theoretically contains \textit{high-level} features of the dataset. However, as we now want to deal with audio signals, we should consider a 
finer representation. Indeed, two similar signals can have slightly varying phases and result in totally different waveforms. Therefore, the reconstruction loss described above \ref{loss1}, if calculated from the raw waveform, penalizes the learned model because the phase variations are not included in the representation. This can corrupt the training process and induce low-level variations of the audio signal in the latent space, which are not relevant for sound rendering. To fix this, we can proceed the \textit{representation learning}. We will use the multiscale spectral distance $S$ described by Engel et al. \cite{engelDDSPDifferentiableDigital2020} in order to estimate the distance between the synthesized and real waveforms. We will therefore use an amplitude spectrum-based distance, which will take into account the phase reconstruction errors and contain the characteristic information of the signal.
We train the encoder and the decoder with the following loss :

\begin{equation}
    \mathcal{L}_{vae} (\textbf{x}) \,=\, \mathbb{E}_{\bm\hat{\textbf{x}} \sim p(\textbf{x}|\textbf{z})}[S(\textbf{x},\bm\hat {\textbf{x}})] \,+\, \beta \mathcal{D}_{\text{KL}}(q_\phi (\textbf{z}|\textbf{x})||p(\textbf{z})).
\end{equation}

Note that there is a second step whose purpose is to improve the audio quality and its natural appearance (see annex for more details on RAVE).



The final objective is to enable the model to acquire condensed representations of the data. In this process, meaningful latent factors exhibit the greatest KL divergence from the prior distribution, whereas less informative latent factors have a KL divergence approaching zero, as outlined by Higgins et al. \cite{higginsVVAELEARNINGBASIC2017}. A post training analysis allows us to identify the most informative part of  the latent representation, and in the end, to reduce its dimensionality to $8$.



%\label{headings}


\section{Proposed Method}

% \subsection{Method}
% Décrire l'achitecture ?
% - architecture du modèle
% - transformation des données
% - reconstruction de phase


\subsection{Audio representation}

One of the issues raised in and by the RAVE paper \cite{caillonRAVEVariationalAutoencoder2021} is the possibly distant representation in the latent space of similar sounding audio. This is caused by the "big" differences of the representation of the temporal waveform for "slight" phase differences. To address this problem, in our approach, we decided to feed to our model the magnitude of Melspectrograms. This makes for similar sounding audio to be close in the latent space, as phase information is cut. However, this causes problems for reconstruction. Indeed, the model now produces the magnitude of MelSpectrogram without any phase information. The choice of Melspectrograms over a normal spectrogram is to emphasize the importance of human perception. We also apply a Log1p function to the Melspectrograms to limit the range of values of the data. 
To reconstruct proper audio from the output of the model, we must create phase information. To do so, we use either the Griffin-Lim algorithm \cite{griff1984} or the Phase Gradient Heap Integration (PGHI) algorithm \cite{pghi2017}. These alogrithms allows us to rebuild phase information from magnitude spectrograms. The Griffin-Lim alogrithm is an iterative algorithm whilst the PGHI algorithm is not. The more performing algorithm will be kept for the final model. 

\subsection{Model}

Wanting our output to have temporal coherence, we take inspiration from the RAVE model \cite{caillonRAVEVariationalAutoencoder2021}, in which 1D convolution are done over the magnitude Melspectrograms. The Mel channels of the spectrograms are used as channels for the convolution, therefore the convolution strides of the time bins and moves through the frequency channels. This creates a latent time dimension. We then create our means and standard deviations from the latent time and features. The model we propose consists of five convolution stages for the encoder, with a leaky ReLU activation function and batch normalization for each stage. The decoder is has a mirror architecture of the encoder with the replacements of transposed convolutions instead of the convolutions. There are also two parallel fully connected layers to produce the means and standard deviations of our latent space. Similarly, there is one fully connected layer to connect the latent space to the entry of the decoder. A detailed summary of the model architecture can be found in the appendix.A summary of our model architecture is given in the appendix \ref{} and a visualization of the full process is given below:

\begin{figure}[ht]
    \centering
    \includesvg
    \caption{Caption}
    \label{fig:enter-label}
\end{figure}


\section{Experiments}
\subsection{Experiments on MNIST}

VAE are a complex model, with several important hyperparameters. Because it can be difficult to visualize the influence of each one of the parameters on a temporal audio model, we first conducted experiments using a simple image VAE trained on the MNIST dataset.

We used simple encoders and decoders, each consisting of two fully connected layers, paired with ReLu or Sigmoid (see appendix \ref{subsec:VAE MNIST archi}). We used a Mean Squared Error as the reconstruction loss, Adam for the optimizer with a learning rate of $1e-3$.

\subsubsection{Influence of the number of latent dimensions}

The first important parameter in a VAE is the dimension of its latent space $n_{l}$.
The higher the dimension, the more complex the VAE. Therefore using a high latent dimension can lead to better reconstruction, but is also harder to control due to the high number of parameters.

As shown in the following paragraph \ref{subsub:beta}, $\beta = 0.5$ seems appropriate to differ from a standard autoencoder ($\beta=0$), while not getting too blurry because of the regularisation.

Figure \ref{fig:n_l influence reconstruction} illustrates this influence: reconstruction is globally better with an increased latent dimension. %TODO insert loss

\begin{figure}
  \centering
  \begin{subfigure}[b]{.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{report/images/lat2/vae_0.5_0.5_ep100_lat2_hid512_2201-0938.pt_original.png}
    \caption{Original test images}
    \label{subfig:original mnist}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{report/images/lat2/vae_0.5_0.5_ep100_lat2_hid512_2201-0938.pt_reconstructed.png}
    \caption{Reconstructed images, $n_l = 2$}
    \label{subfig:recons n_l=2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{report/images/lat4/vae_0.5_0.5_ep100_lat4_hid512_2201-1012.pt_reconstructed.png}
    \caption{Reconstructed images, $n_l = 4$}
    \label{subfig:recons n_l=4}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{report/images/lat8/vae_0.5_0.5_ep100_lat8_hid512_2201-1114.pt_reconstructed.png}
    \caption{Reconstructed images, $n_l = 8$}
    \label{subfig:recons n_l=8}
  \end{subfigure}
  \caption{Influence of $n_l$ over the reconstruction. $\beta=0.5$, results after 100 epochs}
  \label{fig:n_l influence reconstruction}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[b]{.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{report/images/lat2/vae_0.5_0.5_ep100_lat2_hid512_2201-0938.pt_latent_dim.png}
    \caption{Latent dimensions, $n_l = 2$}
    \label{subfig:latent n_l=2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{report/images/lat4/vae_0.5_0.5_ep100_lat4_hid512_2201-1012.pt_latent_dim.png}
    \caption{Reconstructed images, $n_l = 4$}
    \label{subfig:latent n_l=4}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{report/images/lat8/vae_0.5_0.5_ep100_lat8_hid512_2201-1114.pt_latent_dim.png}
    \caption{Reconstructed images, $n_l = 8$}
    \label{subfig:latent n_l=8}
  \end{subfigure}
  \caption{Influence of $n_l$ over the reconstruction. $\beta=0.5$, results after 100 epochs}
  \label{fig:n_l influence latent}
\end{figure}


\subsubsection{Influence of $\beta$ and posterior collapse}
\label{subsub:beta}

Equation \eqref{eq:loss1beta} features a regularization term, the size of which is proportional to $\beta$ 

Tests with $n_{latent} = 8$, $beta \in \{0, 0.5, 1, 2\}$



\subsubsection{$\beta$ warm-up}
% Posterior collapse, tradeoff entre reconstruction et régularisation (quand beta est trop élevé notamment)





\subsection{Results}
% 6 - Results/Discussion



% Mettre image de départ et reconstruction pour minet
% here explain your difficulties 
% also highlight the dataset problem (too complex even for RAVE)  
% you can write your ideas to improve the model and the things you would like to test if you had more time 

As highlighted in the previous section, there is a tradeoff between reconstruction accuracy and latent space regularisation : careful refining of the $\beta$ and/or warmup hyperparameters is needed to achieve satisfactory results.

Going back to our model, we train it on a pruned dataset comprised of audio segments of 1.5 seconds, based on the Freesound Loop Dataset\cite{ramires2020}. All of the audio segments are sampled at 44100 Hz. The audio segments are varied in genres with a slight majority of techno loops.
We start by training the model with $\beta$ fixed to 0, in order to determine the optimal reconstruction epoch. Then, we retrained the model, this time using the beta-warmup procedure described earlier, in order to obtain separation of latent dimensions.

With our trained weights, we are now able to reconstruct audio samples from the dataset. The phase information is obtained by either copying the original phase, or by reconstructing it by the methods described previously. We are also able to generate novel samples by sampling points in the latent space.

Reconstructed examples are available in the \href{https://github.com/estevep/Temporal-Audio-VAE/tree/main/examples}{examples subdirectory of our repository}. Spectrograms of the original example (\verb|original.wav|) and their reconstructed version using our method(\verb|temporal_audio_vae.wav|), RAVE with GAN (\verb|rave.wav|) and without (\verb|rave_vae.wav|) are reproduced in figure \ref{figure:reconstruction}. Additionally, and not reproduced here, we provide an example with reconstructed phase (\verb|temporal_audio_vae_phase.wav|), and a random exploration of the latent space (\verb||).

As we can see in the reconstructed examples, our method\ref{figure:reco-ours}, while retaining some time and frequency coherence, performs poorly compared to RAVE with VAE only\ref{figure:reco-ravenogan}. This illustrates the difficulties we had with defining both the mel-spectrogram resolution (both in time and frequency) and the final encoder / decoder architecture, which are highly coupled.

Furthermore, listening to the example with reconstructed phase, we can hear a significant loss of quality compared to the one with copied phase, despite it being generated with quite a high number of griffin-lim iterations. However, during isolated testing of the mel-spectrogram forward and backward transformation, we observed that the resolution and parameters chosen can provide reconstructions of sufficient quality, including with algorithmic phase reconstruction. Thus, We suspect this illustrates an inherent problem within the model architecture.

Finally, we note that even RAVE with GAN fails to attain an acceptable reconstruction of the original. We attribute this to the inherent complexity of the chosen dataset.

In order to improve upon our results, we can identify some key points to address: 
\begin{itemize}
    \item Future improvement efforts should focus on refining the model architecture and the dimensions of the latent space rather than the data transformation procedure.
    \item A more thorough pruning and cleaning of the dataset would also enable easier reconstruction for both our approach and RAVE, and provide more insightful comparisons.
    \item Finally, due to time constraints, we were not able to assess nor exploit the latent space regularization in our trained model. After addressing the two previous points, and hopefully obtaining comparable reconstructions to rave without GAN, further exploration of this aspect is needed.
\end{itemize}

\begin{figure}[h!]%
\centering
\begin{subfigure}[t]{0.4\textwidth}
\centering
\includegraphics[width=\textwidth]{examples/original.wav.png}
\caption{Original}
\label{figure:reco-original}
\end{subfigure} \hfill
\begin{subfigure}[t]{0.4\textwidth}
\centering
\includegraphics[width=\textwidth]{examples/temporal_audio_vae.wav.png}
\caption{Ours}
\label{figure:reco-ours}
\end{subfigure} \hfill

\begin{subfigure}[t]{0.4\textwidth}
\centering
\includegraphics[width=\textwidth]{examples/rave_vae.wav.png}
\caption{RAVE (VAE only)}
\label{figure:reco-ravenogan}
\end{subfigure} \hfill
\begin{subfigure}[t]{0.4\textwidth}
\centering
\includegraphics[width=\textwidth]{examples/rave.wav.png}
\caption{RAVE (VAE+GAN)}
\label{figure:reco-ravegan}
\end{subfigure}

\caption[Hello]{Spectrograms of reconstructed audio from dataset}
\label{figure:reconstruction}
\end{figure}

% Unfortunately, this model and training scheme produces poor results: the reconstruction we obtain is far from the expected ground truth, and the generated sounds, if recognizable, are quite metallic and distorted in character.

% Some key points to address in order to achieve better reconstruction include the dimensions of the mel spectrogram, the depth of the encoder/decoder stack. The training procedure also sometimes showed numeric instability, resulting in NaN values creeping through and ultimately crashing the training, preventing us from training for a sufficient number of epochs.



%\section{Citations, figures, tables}
%\label{others}
% \end{comment}
\begin{comment}
    

    These instructions apply to everyone.
    
    
    \subsection{Citations within the text}
    
    
    The \verb+natbib+ package will be loaded for you by default.  Citations may be
    author/year or numeric, as long as you maintain internal consistency.  As to the
    format of the references themselves, any style is acceptable as long as it is
    used consistently.
    
    
    The documentation for \verb+natbib+ may be found at
    \begin{center}
      \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
    \end{center}
    Of note is the command \verb+\citet+, which produces citations appropriate for
    use in inline text.  For example,
    \begin{verbatim}
       \citet{hasselmo} investigated\dots
    \end{verbatim}
    produces
    \begin{quote}
      Hasselmo, et al.\ (1995) investigated\dots
    \end{quote}
    
    
    If you wish to load the \verb+natbib+ package with options, you may add the
    following before loading the \verb+neurips_2023+ package:
    \begin{verbatim}
       \PassOptionsToPackage{options}{natbib}
    \end{verbatim}
    
    
    If \verb+natbib+ clashes with another package you load, you can add the optional
    argument \verb+nonatbib+ when loading the style file:
    \begin{verbatim}
       \usepackage[nonatbib]{neurips_2023}
    \end{verbatim}
    
    
    As submission is double blind, refer to your own published work in the third
    person. That is, use ``In the previous work of Jones et al.\ [4],'' not ``In our
    previous work [4].'' If you cite your other papers that are not widely available
    (e.g., a journal paper under review), use anonymous author names in the
    citation, e.g., an author of the form ``A.\ Anonymous'' and include a copy of the anonymized paper in the supplementary material.
    
    
    \subsection{Footnotes}
    
    
    Footnotes should be used sparingly.  If you do require a footnote, indicate
    footnotes with a number\footnote{Sample of the first footnote.} in the
    text. Place the footnotes at the bottom of the page on which they appear.
    Precede the footnote with a horizontal rule of 2~inches (12~picas).
    
    
    Note that footnotes are properly typeset \emph{after} punctuation
    marks.\footnote{As in this example.}
    
    
    \subsection{Figures}
    
    
    \begin{figure}
      \centering
      \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
      \caption{Sample figure caption.}
    \end{figure}
    
    
    All artwork must be neat, clean, and legible. Lines should be dark enough for
    purposes of reproduction. The figure number and caption always appear after the
    figure. Place one line space before the figure caption and one line space after
    the figure. The figure caption should be lower case (except for first word and
    proper nouns); figures are numbered consecutively.
    
    
    You may use color figures.  However, it is best for the figure captions and the
    paper body to be legible if the paper is printed in either black/white or in
    color.
    
    
    \subsection{Tables}
    
    
    All tables must be centered, neat, clean and legible.  The table number and
    title always appear before the table.  See Table~\ref{sample-table}.
    
    
    Place one line space before the table title, one line space after the
    table title, and one line space after the table. The table title must
    be lower case (except for first word and proper nouns); tables are
    numbered consecutively.
    
    
    Note that publication-quality tables \emph{do not contain vertical rules.} We
    strongly suggest the use of the \verb+booktabs+ package, which allows for
    typesetting high-quality, professional tables:
    \begin{center}
      \url{https://www.ctan.org/pkg/booktabs}
    \end{center}
    This package was used to typeset Table~\ref{sample-table}.
    
    
    \begin{table}
      \caption{Sample table title}
      \label{sample-table}
      \centering
      \begin{tabular}{lll}
        \toprule
        \multicolumn{2}{c}{Part}                   \\
        \cmidrule(r){1-2}
        Name     & Description     & Size ($\mu$m) \\
        \midrule
        Dendrite & Input terminal  & $\sim$100     \\
        Axon     & Output terminal & $\sim$10      \\
        Soma     & Cell body       & up to $10^6$  \\
        \bottomrule
      \end{tabular}
    \end{table}
    
    \subsection{Math}
    Note that display math in bare TeX commands will not create correct line numbers for submission. Please use LaTeX (or AMSTeX) commands for unnumbered display math. (You really shouldn't be using \$\$ anyway; see \url{https://tex.stackexchange.com/questions/503/why-is-preferable-to} and \url{https://tex.stackexchange.com/questions/40492/what-are-the-differences-between-align-equation-and-displaymath} for more information.)
    
    \subsection{Final instructions}
    
    Do not change any aspects of the formatting parameters in the style files.  In
    particular, do not modify the width or length of the rectangle the text should
    fit into, and do not change font sizes (except perhaps in the
    \textbf{References} section; see below). Please note that pages should be
    numbered.
\end{comment}

\printbibliography
% \bibliography



\section{Appendixes}

\subsection{VAE on MNIST architecture Simple encoder}
\label{subsec:VAE MNIST archi}

This is the architecture of the image VAE trained on MNIST, here with $n_l = 2$
\begin{lstlisting}
VAE(
  (encoder): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=784, out_features=512, bias=True)
    (2): ReLU()
    (3): Linear(in_features=512, out_features=512, bias=True)
    (4): ReLU()
  )
  (decoder): Sequential(
    (0): Linear(in_features=2, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=784, bias=True)
    (3): Sigmoid()
  )
  (mu): Linear(in_features=512, out_features=2, bias=True)
  (sigma): Sequential(
    (0): Linear(in_features=512, out_features=2, bias=True)
    (1): Softplus(beta=1, threshold=20)
  )
)
\end{lstlisting}

\subsection{Our architecture}
\begin{lstlisting}
MelSpecVAE(
  (encoder): Sequential(
    (0): Conv1d(512, 256, kernel_size=(8,), stride=(2,), padding=(3,))
    (1): LeakyReLU(negative_slope=0.01)
    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): Conv1d(256, 128, kernel_size=(8,), stride=(2,), padding=(3,))
    (4): LeakyReLU(negative_slope=0.01)
    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): Conv1d(128, 64, kernel_size=(8,), stride=(2,), padding=(3,))
    (7): LeakyReLU(negative_slope=0.01)
    (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): Conv1d(64, 32, kernel_size=(8,), stride=(2,), padding=(3,))
    (10): LeakyReLU(negative_slope=0.01)
    (11): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): Conv1d(32, 16, kernel_size=(8,), stride=(2,), padding=(3,))
    (13): LeakyReLU(negative_slope=0.01)
    (14): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (15): Flatten(start_dim=1, end_dim=-1)
  )
  (decoder): Sequential(
    (0): Linear(in_features=16, out_features=128, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Unflatten(dim=1, unflattened_size=torch.Size([16, 8]))
    (3): ConvTranspose1d(16, 32, kernel_size=(8,), stride=(2,), padding=(3,))
    (4): LeakyReLU(negative_slope=0.01)
    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ConvTranspose1d(32, 64, kernel_size=(8,), stride=(2,), padding=(3,))
    (7): LeakyReLU(negative_slope=0.01)
    (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (9): ConvTranspose1d(64, 128, kernel_size=(8,), stride=(2,), padding=(3,))
    (10): LeakyReLU(negative_slope=0.01)
    (11): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (12): ConvTranspose1d(128, 256, kernel_size=(8,), stride=(2,), padding=(3,))
    (13): LeakyReLU(negative_slope=0.01)
    (14): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (15): ConvTranspose1d(256, 512, kernel_size=(8,), stride=(2,), padding=(3,))
    (16): LeakyReLU(negative_slope=0.01)
    (17): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (mu): Sequential(
    (0): Linear(in_features=128, out_features=16, bias=True)
  )
  (sigma): Sequential(
    (0): Linear(in_features=128, out_features=16, bias=True)
    (1): Softplus(beta=1, threshold=20)
  )
)
\end{lstlisting}

\subsection{RAVE architecture}

% @Enguérand ça peut aller ici ce que tu enlèves de l'intro

\subsection{Task distribution}

% TODO remplir

\begin{itemize}
    \item[•] Paul
    \begin{itemize}
        \item[$\circ$] 
        \item[$\circ$] 
        \item[$\circ$] 
    \end{itemize}
    \item[•] Antoine
    \begin{itemize}
        \item[$\circ$] 
        \item[$\circ$] 
        \item[$\circ$] 
    \end{itemize}
    \item[•] Etienne
    \begin{itemize}
        \item[$\circ$] 
        \item[$\circ$] 
        \item[$\circ$] 
    \end{itemize}
    \item[•] Enguérand
    \begin{itemize}
        \item[$\circ$] 
        \item[$\circ$] 
        \item[$\circ$] 
    \end{itemize}



\end{itemize}

% \section*{References}



% [1] Antoine Caillon and Philippe Esling.  Rave: A variational autoencoder for fast and high-quality neural audio synthesis. \textit{arXiv preprint arXiv:2111.05011}, 2021.


% [2] Esse Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan Gulrajani, Chris Donahue and Adam Roberts. Gan- synth: Adversarial neural audio synthesis. \textit{arXiv preprint arXiv:1902.08710}, 2019.


% [3] Jesse Engel, Lamtharn Hantrakul, Chenjie Gu and Adam Roberts. Ddsp: Differentiable digital signal processing. \textit{arXiv preprint arXiv:2001.04643}, 2020.

% [4] Irina Higgins, Loïc Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew M Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational frame- work. \textit{In ICLR}, 2017.


% [5] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. \textit{arXiv:1312.6114}, 2013. 



% [6] Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de Brebisson, Yoshua Bengio, and Aaron Courville. MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis. \textit{arXiv, 10 2019}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}