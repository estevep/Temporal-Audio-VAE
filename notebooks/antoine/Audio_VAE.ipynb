{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distrib\n",
    "import numpy as np\n",
    "import lmdb\n",
    "import torchaudio\n",
    "import librosa\n",
    "from udls.generated import AudioExample\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, encoding_dim):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoding_dims = encoding_dim\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(AE):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, encoding_dims, latent_dims):\n",
    "        super(VAE, self).__init__(encoder, decoder, encoding_dims)\n",
    "        self.latent_dims = latent_dims\n",
    "        self.mu = nn.Sequential(nn.Linear(self.encoding_dims, self.latent_dims))\n",
    "        self.sigma = nn.Sequential(nn.Linear(self.encoding_dims, self.latent_dims), nn.Softplus())\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \n",
    "        x_encoded = self.encoder(x)\n",
    "\n",
    "        mu = self.mu(x_encoded)\n",
    "        sigma = self.sigma(x_encoded)\n",
    "\n",
    "        return (mu, sigma), x_encoded.shape\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode the inputs\n",
    "        z_params, x_encoded_shape = self.encode(x)\n",
    "        print(f\"{x_encoded_shape=}\")\n",
    "        # Obtain latent samples and latent loss\n",
    "        mu, _ = z_params\n",
    "        print(mu.shape)\n",
    "        z_tilde, kl_div = self.latent(z_params)\n",
    "        # Decode the samples\n",
    "        z_tilde = z_tilde.reshape(x_encoded_shape)\n",
    "        x_tilde = self.decode(z_tilde)\n",
    "        return x_tilde.reshape(x.shape), kl_div\n",
    "    \n",
    "    def latent(self, z_params):\n",
    "        \n",
    "        normal = distrib.Normal(loc=0., scale=1.)\n",
    "        mu, sigma = z_params\n",
    "        kl_div = torch.sum(1 + torch.log(sigma**2) - mu**2 - 2*(sigma**2))/2\n",
    "        z = mu + sigma * normal.sample(sigma.shape)\n",
    "    \n",
    "        return z, kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_encoder_decoder(nin, n_latent = 16, n_hidden = 512, n_classes = 1):\n",
    "    # Encoder network\n",
    "    encoder = nn.Sequential(\n",
    "        nn.Conv1d(128, 64,7,2), nn.LeakyReLU(), nn.BatchNorm1d(64),\n",
    "        nn.Conv1d(64, 32,7,2), nn.LeakyReLU(), nn.BatchNorm1d(32),\n",
    "        nn.Conv1d(32, 16,7,4), nn.LeakyReLU(), nn.BatchNorm1d(16),\n",
    "        nn.Flatten()\n",
    "\n",
    "\n",
    "    )\n",
    "    # Decoder network\n",
    "    decoder = nn.Sequential(\n",
    "        nn.ConvTranspose1d(16,32,7,4), nn.LeakyReLU(), nn.BatchNorm1d(32),\n",
    "        nn.ConvTranspose1d(32,64,7,4), nn.LeakyReLU(), nn.BatchNorm1d(64),\n",
    "        nn.ConvTranspose1d(64,128,7,4), nn.LeakyReLU(), nn.BatchNorm1d(128),\n",
    "    )\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.ones((64, 128, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Bernoulli or Multinomial loss\n",
    "num_classes = 1\n",
    "# Number of hidden and latent\n",
    "n_hidden = 992\n",
    "n_latent = 16\n",
    "# Compute input dimensionality\n",
    "nin = test.shape[1] * test.shape[2]\n",
    "# Construct encoder and decoder\n",
    "encoder, decoder = construct_encoder_decoder(nin, n_hidden = n_hidden, n_latent = n_latent, n_classes = num_classes)\n",
    "# Build the VAE model\n",
    "model = VAE(encoder, decoder, n_hidden, n_latent)\n",
    "# Construct the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_encoded_shape=torch.Size([64, 992])\n",
      "torch.Size([64, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[64, 992]' is invalid for input of size 1024",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\MrGoo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MrGoo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[41], line 30\u001b[0m, in \u001b[0;36mVAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m z_tilde, kl_div \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent(z_params)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Decode the samples\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m z_tilde \u001b[38;5;241m=\u001b[39m \u001b[43mz_tilde\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_encoded_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m x_tilde \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(z_tilde)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_tilde\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape), kl_div\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[64, 992]' is invalid for input of size 1024"
     ]
    }
   ],
   "source": [
    "model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m full_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Forward pass: compute predicted y by passing x to the model.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtrain_loader\u001b[49m):\n\u001b[0;32m      6\u001b[0m     full_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_step(model, x, optimizer, beta \u001b[38;5;241m=\u001b[39m betas[epoch])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#for i, (x, _) in enumerate(valid_loader):\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#    train_step(model, x, optimizer)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range(1, epochs + 1):\n",
    "    full_loss = torch.Tensor([0])\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    for i, (x, _) in enumerate(train_loader):\n",
    "        full_loss += train_step(model, x, optimizer, beta = betas[epoch])\n",
    "    #for i, (x, _) in enumerate(valid_loader):\n",
    "    #    train_step(model, x, optimizer)\n",
    "    if (epoch % 10 == 0):\n",
    "        print('Epoch: {}, Test set ELBO: {}'.format(epoch, full_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
